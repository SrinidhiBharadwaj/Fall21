{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f55b7c4c",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e210ec02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dec15a",
   "metadata": {},
   "source": [
    "### Problem 2: i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242891f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "car = cv2.imread(\"Car.tif\")\n",
    "car = cv2.cvtColor(car, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e69c50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "car_fft = np.fft.fft2(car)\n",
    "car_shift = np.fft.fftshift(car_fft) \n",
    "magnitude_spectrum = 20*np.log(np.abs(car_shift))\n",
    "magnitude_spectrum = magnitude_spectrum.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2526bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10  #Filter order\n",
    "D_0 = 20 #Radius (Empirically found)\n",
    "\n",
    "notch_filter = np.zeros((512, 512), dtype=np.float64)\n",
    "\n",
    "#Calculating the values in the notch_filter according to the equation\n",
    "\n",
    "x_axis = np.linspace(-256,255,512)\n",
    "y_axis = np.linspace(-256,255,512)\n",
    "[u,v] = np.meshgrid(x_axis,y_axis)\n",
    "eps = 10**-7\n",
    "filt_order = 2*n\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        D1 = math.sqrt((u[i, j]-90)**2 + (v[i, j]+160)**2) + eps\n",
    "        D1_k = math.sqrt((u[i, j]+90)**2 + (v[i, j]-160)**2)+ eps\n",
    "        \n",
    "        D2 = math.sqrt((u[i, j]-90)**2 + (v[i, j]+160)**2)+ eps\n",
    "        D2_k = math.sqrt((u[i, j]+90)**2 + (v[i, j]-160)**2)+ eps\n",
    "                        \n",
    "        D3 = math.sqrt((u[i, j]-90)**2 + (v[i, j]-160)**2)+ eps\n",
    "        D3_k = math.sqrt((u[i, j]+90)**2 + (v[i, j]+160)**2)+ eps\n",
    "        \n",
    "        D4 = math.sqrt((u[i, j]-90)**2 + (v[i, j]-160)**2)+ eps\n",
    "        D4_k = math.sqrt((u[i, j]+90)**2 + (v[i, j]+160)**2)+ eps\n",
    "                \n",
    "        val1 = (1/(1+(D_0/D1)**filt_order)) * (1/(1+(D_0/D1_k)**filt_order))\n",
    "        val2 = (1/(1+(D_0/D2)**filt_order)) * (1/(1+(D_0/D2_k)**filt_order))\n",
    "        val3 = (1/(1+(D_0/D3)**filt_order)) * (1/(1+(D_0/D3_k)**filt_order))\n",
    "        val4 = (1/(1+(D_0/D4)**filt_order)) * (1/(1+(D_0/D4_k)**filt_order))\n",
    "        notch_filter[i, j] = val1*val2*val3*val4                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199329e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter the given input image that is in the frequency domain\n",
    "filtered_out = car_shift * notch_filter\n",
    "shifted = np.fft.ifftshift((filtered_out))\n",
    "spatial = np.fft.ifft2(shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01dd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(20, 10))\n",
    "ax1= fig.add_subplot(2,2,1)\n",
    "ax1.title.set_text(\"Original Image\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(car, cmap='gray')\n",
    "axs[0, 0].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "ax1= fig.add_subplot(2,2,2)\n",
    "ax1.title.set_text(\"Magnitude spectrum\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(magnitude_spectrum)\n",
    "axs[0, 1].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "ax1= fig.add_subplot(2,2,3)\n",
    "ax1.title.set_text(\"Butterworth notch filter\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(notch_filter)\n",
    "axs[1, 0].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "ax1= fig.add_subplot(2,2,4)\n",
    "ax1.title.set_text(\"Filtered Image\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(spatial_out, cmap='gray')\n",
    "axs[1, 1].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98d3b7",
   "metadata": {},
   "source": [
    "### Problem 2: ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd88a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "street = cv2.imread(\"Street.png\")\n",
    "street = cv2.cvtColor(street, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470065e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FFT calculation using numpy's fft function\n",
    "#fft is calculated, shifted and scaled with as 10l*og(F(u,v))\n",
    "street_fft = np.fft.fft2(padded_street)\n",
    "street_shift = np.fft.fftshift(street_fft) \n",
    "magnitude_spectrum = 20*np.log(np.abs(street_shift))\n",
    "magnitude_spectrum = magnitude_spectrum.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f68b60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = (6) # Filter order\n",
    "D_0 = (15) #Radius\n",
    "\n",
    "notch_filter = np.zeros((512, 512), dtype=np.float64)\n",
    "\n",
    "x_axis = np.linspace(-256,255,512)\n",
    "y_axis = np.linspace(-256,255,512)\n",
    "[u,v] = np.meshgrid(x_axis,y_axis)\n",
    "eps = 10**-5\n",
    "filt_order = 2*n\n",
    "for i in range(512):\n",
    "    for j in range(512):\n",
    "        D1 = math.sqrt((u[i, j]-0)**2 + (v[i, j]-164)**2) + eps\n",
    "        D1_k = math.sqrt((u[i, j]+0)**2 + (v[i, j]+164)**2)+ eps\n",
    "        \n",
    "        D2 = math.sqrt((u[i, j]-164)**2 + (v[i, j]-0)**2)+ eps\n",
    "        D2_k = math.sqrt((u[i, j]+164)**2 + (v[i, j]+0)**2)+ eps\n",
    "\n",
    "        \n",
    "        val1 = (1/(1+(D_0/D1)**filt_order)) * (1/(1+(D_0/D1_k)**filt_order))\n",
    "        val2 = (1/(1+(D_0/D2)**filt_order)) * (1/(1+(D_0/D2_k)**filt_order))\n",
    "\n",
    "        notch_filter[i, j] = val1*val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b292600",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_out = street_shift * notch_filter\n",
    "shifted = np.fft.ifftshift((filtered_out))\n",
    "spatial = np.fft.ifft2(shifted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09434d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2,2, figsize=(20, 10))\n",
    "ax1= fig.add_subplot(2,2,1)\n",
    "ax1.title.set_text(\"Original Image\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(street, cmap='gray')\n",
    "axs[0, 0].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "ax1= fig.add_subplot(2,2,2)\n",
    "ax1.title.set_text(\"Magnitude spectrum\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(magnitude_spectrum)\n",
    "axs[0, 1].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "ax1= fig.add_subplot(2,2,3)\n",
    "ax1.title.set_text(\"Butterworth notch filter\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(notch_filter)\n",
    "axs[1, 0].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "ax1= fig.add_subplot(2,2,4)\n",
    "ax1.title.set_text(\"Filtered Image\")\n",
    "ax1.title.set_size(15)\n",
    "ax1.axis('off')\n",
    "ax = ax1.imshow(spatial_out, cmap='gray')\n",
    "axs[1, 1].axis('off')\n",
    "plt.colorbar(ax)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a2b55b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81a4c6c8",
   "metadata": {},
   "source": [
    "### Problem - 3\n",
    "\n",
    "__ii)__ How many images and batches are used to train the network?\n",
    "\n",
    "Ans: __There are 50,000 images in the training set. Batch size of 4 is used for training purposes.__\n",
    "\n",
    "__iii)__ Do we normalize the images? What do we do in the example?\n",
    "\n",
    "Ans: __Yes, the images are being normalized.__\n",
    "\n",
    "__iv)__ The losses are dropping! Can you plot out the training loss?\n",
    "\n",
    "![Loss Versus Epoch](LossvsEpoch.png)\n",
    "\n",
    "\n",
    "__v)__ Now the network is done training. Can you check some successful cases and some failure cases\n",
    "(show some images classified by the network)?\n",
    "    \n",
    "__Test images and classes__\n",
    "\n",
    "![Test images and classes](TestClasses.png)\n",
    "\n",
    "__Predicted__\n",
    "\n",
    "![Predictions](Predictions.png)\n",
    "\n",
    "__vi)__ Can you visualize the output of the 1st layer of CNN using one image from the training set?\n",
    "\n",
    "Ans: __Ouput of the first layer of CNN is as shown below. Output consists of 6 channels and each of the channels are of size 28x28. Each channels are plotted separately. \"Net\" class' conv1 attribute is utilized for plotting.__\n",
    "\n",
    "![Code for generating the convolution output](Conv1Code.png)\n",
    "\n",
    "![Convolution and ReLu output](Conv1Output.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
